"""
Created on Mon Feb 27 19:52:54 2023

@author: micha
"""
import parser
import numpy as np
import math
import pandas as pd
import scipy.sparse.linalg as sla
import itertools
import matplotlib.pyplot as plt
from numpy import size
from sklearn import preprocessing
import time
import numpy as np
import pandas as pd
import scipy.sparse.linalg as sla
from random import sample
from numpy import inf
import itertools
import matplotlib.pyplot as plt
from numpy import size
from sklearn import preprocessing
from os.path import dirname, join as pjoin
import scipy.io 

np.seterr(divide = 'ignore') 
G = pd.read_csv("G_dolphins.csv", header=None).values
mat = G
heuristic=1
num_pairs=2


def isSymmetric(mat):
    N = len(mat)
    for i in range(N):
        for j in range(N):
            if (mat[i][j] != mat[j][i]):
                return False
    return True
def avgClusteringCoefficient(graph):
    if isSymmetric(graph):
        graph[graph != 0] = 1
        deg = np.sum(graph)   #determine node degrees
        cn = np.diag(graph* np.triu(graph)*graph) #Determine Number of triangles in each node
        # local clustering coefficent of each node
        c = np.zeros(np.size(deg))
        c[deg > 1] = np.divide(2 * cn[deg > 1] , np.multiply(deg[deg >1], deg[deg>1] - 1))
        acc = np.mean(c[deg >1])
    return acc, c
def rate_distortion(G, heuristic, num_pairs):
    #size of network
    N = len(G)
    E = int(np.sum(G[:])/2)

    #variables to save
    S = np.zeros((1,N), dtype=float) #Upper bound on entropy rate after clustering
    S_low = np.zeros((1,N), dtype=float)#Lower bound on entropy rate
    clusters = [[] for _ in range(N)] #clusters{n} lists the nodes in each of the n clusters
    Gs = [[] for _ in range(N)] #Gs{n} is the joint transition probability matrix for n clusters

    #transition proability matrix
    P_old = np.divide(G, np.transpose(np.tile(sum(G), (N,1))))

    #Compute steady-state distribution
    D, p_ss = sla.eigs(np.transpose(P_old),return_eigenvectors=True)  #works for all networks     #need to rewrite this section
    D = np.round(D.real, decimals=4)
    p_ss = p_ss.real
    ind = int(np.round(np.max(D))) - 1 
    p_ss = np.round(p_ss[:,ind]/sum(p_ss[:,ind]), decimals=4)

    #p_ss = sum(G,2)/sum(G(:)); % Only true for undirected networks
    p_ss_old = p_ss

    #calculate initial entropy:
    logP_old = np.log2(P_old)
    logP_old[logP_old == -inf] = 0
    S_old = np.round(-np.sum(np.multiply(p_ss_old, np.sum(np.multiply(P_old, logP_old), axis=1))), decimals = 4)
    P_joint = np.round(np.multiply(P_old, np.transpose(np.tile(p_ss_old, (N,1)))), decimals=4)
    P_low = P_old
    #Record inital values
    S[:,-1] = S_old
    S_low[:,-1] = S_old
    clusters[-1] = np.transpose(np.array([[x] for x in range(0, N)]))
    Gs[-1] = G

    #loop over the number of clusterings
    for i in range(N-1,1,-1):
    #different sets of node pairs to try:
        if heuristic == 1:
        #try combining all pairs:
            pairs = np.array(list(itertools.combinations([x for x in range(i+1)], 2)))
            I = pairs[:,0]
            J = pairs[:,1]
        elif heuristic == 2:
            #pick num_pair node pairs at random
            pairs = np.array(list(itertools.combinations([x for x in range(i+1)], 2)))
            inds = sample(size(pairs,1), np.min([num_pairs, size(pairs,1)])); 
            I = pairs(inds,1)
            J = pairs(inds,2)
        else:
            print('Variable "setting" is not properly defined.')

        #number of pairs 
        num_pairs_temp = len(I)
    
        #keep track of all entropies
        S_all = np.zeros((1, num_pairs_temp), dtype=float)

        #loop over the pairs of nodes:
        for ind in range(num_pairs_temp):
            ii = I[ind]
            jj = J[ind]
            inds_not_ij = list(range(0,(ii))) + list(range((ii+1),(jj))) + list(range((jj+1),(i+1)))

            #compute new stationary distribution:           
            p_ss_temp = np.append(p_ss_old[inds_not_ij], p_ss_old[ii] + p_ss_old[jj])
            
            #stopped here
            # Compute new transition probabilities:
            P_temp_1 = np.sum(np.multiply(np.transpose(np.tile(p_ss_old[inds_not_ij], (2,1))), P_old[:, [ii,jj]][inds_not_ij]), axis = 1)
            P_temp_1 = np.round(P_temp_1 / p_ss_temp[0:-1],decimals=4)
            P_temp_2 = np.sum(np.multiply(np.transpose(np.tile(np.r_[p_ss_old[ii],p_ss_old[jj]] , (i-1,1))), P_old[[ii,jj],:][:,inds_not_ij]),axis=0, keepdims=True)
            P_temp_2 = np.round(P_temp_2 / p_ss_temp[-1],decimals=4)
            P_temp_3 = np.sum(np.multiply(np.transpose(np.tile(np.r_[p_ss_old[ii],p_ss_old[jj]], (2,1))), P_old[:,[ii,jj]][[ii,jj], :]))
            P_temp_3 = np.round(P_temp_3 / p_ss_temp[-1],decimals=4)
            
            logP_temp_1 = np.log2(P_temp_1)
            logP_temp_1[logP_temp_1 == -inf] = 0
            logP_temp_2 = np.log2(P_temp_2)
            logP_temp_2[logP_temp_2 == -inf] = 0
            logP_temp_3 = np.log2(P_temp_3)
            logP_temp_3 = np.array([logP_temp_3])
            logP_temp_3[logP_temp_3 == -inf] = 0

            #Compute change in upper bound on mutual information
            d1 = round(-sum(np.multiply(np.multiply(p_ss_temp[0:-1], P_temp_1), logP_temp_1)), 4)
            d2 = round(- p_ss_temp[-1]*np.sum(np.multiply(P_temp_2,logP_temp_2)), 4)
            d3 = round(- p_ss_temp[-1]*P_temp_3*float(logP_temp_3[0]), 4)
            d4 = round(np.sum(np.multiply(p_ss_old, np.multiply(P_old[:,ii], logP_old[:, ii]))), 4)
            d5 = round(np.sum(np.multiply(np.multiply(p_ss_old, P_old[:,jj]), logP_old[:,jj])), 4)
            d6 = round(p_ss_old[ii]*np.sum(np.multiply(P_old[ii,:],logP_old[ii,:])), 4)
            d7 =  round(p_ss_old[jj]*np.sum(np.multiply(P_old[jj,:], logP_old[jj,:])), 4)
            d8 = round(- p_ss_old[ii]*(P_old[ii,ii]*logP_old[ii,ii] + P_old[ii,jj]*logP_old[ii,jj]), 4)
            d9 = round(- p_ss_old[jj]*(P_old[jj,jj]*logP_old[jj,jj] + P_old[jj,ii]*logP_old[jj,ii]), 4)
            dS = d1+d2+d3+d4+d5+d6+d7+d8+d9
            dS = round(dS, 4)
            S_temp = 10000*(S_old + dS)
            
        
               # Keep track of all entropies:
            S_all[:,ind] = S_temp



         # Find minimum entropy:
        [dummy, min_inds] = np.nonzero(S_all == np.min(S_all))
        test1 = list(min_inds)
        min_ind = test1[-1]   #fix
    
        # Save mutual information:
        S_old = S_all[:,min_ind]
        S[:,i-1] = S_old
    
        # Compute old transition probabilities:
        ii_new = int(I[min_ind])
        jj_new = int(J[min_ind])
        
        inds_not_ij = list(range(0,(ii_new))) + list(range((ii_new+1),(jj_new))) + list(range((jj_new+1),(i+1))) 

        p_ss_new = np.append(p_ss_old[inds_not_ij], p_ss_old[ii_new] + p_ss_old[jj_new])
        P_joint = np.multiply(np.transpose(np.tile(p_ss_old, (i+1, 1))), P_old)
        P_joint = np.c_[np.r_[P_joint[:,inds_not_ij][inds_not_ij,:],np.sum(P_joint[[ii_new,jj_new],:][:,inds_not_ij], axis=0, keepdims=True)], 
                         np.append(np.sum((P_joint[:, [ii_new,jj_new]][inds_not_ij]), axis=1),np.sum(P_joint[:,range(ii_new,jj_new+1)][range(ii_new,jj_new+1),:]))]
        
        
        
        P_old = np.divide(P_joint, np.transpose(np.tile(p_ss_new, (i, 1))))
        p_ss_old = p_ss_new
    
        logP_old = np.log2(P_old)
        logP_old[logP_old == -inf] = 0

        # Record clusters and graph:
        clusters[i-1] = list(range(0,ii_new))+list(range(ii_new+1, jj_new))+list(range(jj_new+1, i+2)) 
        clusters[i-1].append([ii_new, jj_new])
        Gs[i-1] = P_joint*2*E
    
        # Compute lower bound on mutual information:
        P_low = np.c_[P_low[:,list(range(0,ii_new))+list(range(ii_new+1, jj_new))+list(range(jj_new+1, i+1))] , P_low[:,ii_new] + P_low[:,jj_new]]

        logP_low = np.log2(P_low)
        logP_low[logP_low == -inf] = 0
        S_low[:,i-1] = np.round(-sum(np.multiply(p_ss , np.sum(np.multiply(P_low, logP_low), axis=1))), decimals=4)

    return(S, S_low, clusters, Gs)


info_struct = pd.read_csv("graphs_info.csv", header=None).values
info = info_struct
test1 = (info[:,2] == 1)
info = test1[test1,:]

families = np.unique(info[:,0]) # get unique rows for first column "family"
num_families = len(families)
graphs_topTrans = np.array(['G_FOLDOC', 'G_Clavier', 'G_SC_2'])

# loop over graph families
for i in range(num_families):
    # load Graphs    pd.csv     need to do
    graph_struct = scipy.io.loadmat('graphs_Protein_samples_undirected.mat')
    graph_names = graph_struct.keys()
    graph_names.remove('__version__')
    graph_names.remove('__header__')
    graph_names.remove('__globals__')
    num_graphs = len(graph_names)
    

    # Different graphs in this family
    
    num_graphs = len(graph_names)
    # Different things to compute
    # Can change lists into a pandas DataFrame to store data better
    rate_distortion_upper = [None] * num_graphs
    rate_distortion_lower = [None] * num_graphs
    avg_outDeg_inside = [None] * num_graphs
    avg_inDeg_inside = [None] * num_graphs
    avg_outDeg_outside = [None] * num_graphs
    avg_inDeg_outside = [None] * num_graphs
    num_edges_inside = [None] * num_graphs
    num_edges_boundary_out = [None] * num_graphs
    num_edges_boundary_in = [None] * num_graphs
    num_edges_outside = [None] * num_graphs
    cluster_size = [None] * num_graphs
    compressibility = [None] * num_graphs
    avg_deg = [None] * num_graphs
    avg_clustering = [None] * num_graphs
    outDeg_het = [None] * num_graphs
    inDeg_het = [None] * num_graphs
    
    # Structure to Save
    measures_struct = {}
    
    # loop over different graphs in this family
    
    for j in range(num_graphs):
        # Load Graph samples     
        Gs = graph_struct[graph_names[j]]
        num_samples = len(Gs)
        # Things to compute for this graph
        rate_distortion_upper_temp = [None] * num_samples
        rate_distortion_lower_temp = [None] * num_samples
        avg_outDeg_inside_temp = [None] * num_samples
        avg_inDeg_inside_temp = [None] * num_samples
        avg_outDeg_outside_temp = [None] * num_samples
        avg_inDeg_outside_temp = [None] * num_samples
        num_edges_inside_temp = [None] * num_samples
        num_edges_boundary_out_temp = [None] * num_samples
        num_edges_boundary_in_temp = [None] * num_samples
        num_edges_outside_temp = [None] * num_samples
        cluster_size_temp = [None] * num_samples
        compressibility_temp = [0] * num_samples
        avg_deg_temp = [0] * num_samples
        avg_clustering_temp = [0] * num_samples
        outDeg_het_temp = [0] * num_samples
        inDeg_het_temp = [0] * num_samples        
        
        for k in range(num_samples):
            # Load Graph samples
            G = Gs[k]
            N = G.shape[0]
            ks_out = G.sum(axis=1)
            ks_in = G.sum(axis=0)
            
            # Compute Simple graph measures
            avg_deg_temp[k] = np.mean(ks_out)
            avg_clustering_temp[k] = avgClusteringCoefficient((G+G.T)>0)
            outDeg_het_temp[k] = np.sum(np.abs(ks_out - ks_out.T))/(N*(N-1)*np.mean(ks_out))
            inDeg_het_temp = np.sum(np.abs(ks_in - np.transpose(ks_in))) / (N * (N - 1) * np.mean(ks_in))
            
            
            # Compute rate-distortion curve
            if np.in1d(graph_names[j], graphs_topTrans):
                 rd_upper, rd_lower, Cs = rate_distortion(G,2,100)  #need to fix
                
            rate_distortion_upper_temp[k] = rd_upper
            rate_distortion_lower_temp[k] = rd_lower
            compressibility_temp[k] = np.mean(rd_upper[-1] - rd_upper)
            
            # Compute statistics about large clusters
            avg_outDeg_inside_temp_temp = np.zeros(N-2)
            avg_inDeg_inside_temp_temp = np.zeros(N-2)
            avg_outDeg_outside_temp_temp = np.zeros(N-2)
            avg_inDeg_outside_temp_temp = np.zeros(N-2)
            num_edges_inside_temp_temp = np.zeros(N-1)
            num_edges_boundary_out_temp_temp = np.zeros(N-1)
            num_edges_boundary_in_temp_temp = np.zeros(N-1)
            num_edges_outside_temp_temp = np.zeros(N-1)
            cluster_size_temp_temp = np.zeros(N)

            cluster_size_temp_temp[0] = N
            cluster_size_temp_temp[-1] = 1

            num_edges_inside_temp_temp[0] = np.sum(G)/2
            num_edges_boundary_out_temp_temp[0] = 0
            num_edges_boundary_in_temp_temp[0] = 0
            num_edges_outside_temp_temp[0] = 0
            
            for l in range(1, N-1):
                C = Cs[l]
    
            # Get largest cluster:
                sizes = [len(x) for x in C]
                ind = sizes.index(max(sizes))
                cluster = C[ind]
                not_cluster = set(range(1, N+1)).difference(cluster)
                
                # Compute things:
                cluster_size_temp_temp[l] = len(cluster)
                avg_outDeg_inside_temp_temp[l-1] = np.mean(ks_out[cluster])
                avg_inDeg_inside_temp_temp[l-1] = np.mean(ks_in[cluster])
                avg_outDeg_outside_temp_temp[l-1] = np.mean(ks_out[not_cluster])
                avg_inDeg_outside_temp_temp[l-1] = np.mean(ks_in[not_cluster])
                num_edges_inside_temp_temp[l] = sum(sum(G[cluster, cluster]))
                num_edges_boundary_out_temp_temp[l] = sum(sum(G[cluster, cluster]))
                num_edges_boundary_in_temp_temp[l] = sum(sum(G[not_cluster, cluster]))
                num_edges_outside_temp_temp[l] = sum(sum(G[not_cluster, not_cluster]))
                    
                    
                    
                    
        avg_outDeg_inside_temp[k] = avg_outDeg_inside_temp_temp
        avg_inDeg_inside_temp[k] = avg_inDeg_inside_temp_temp
        avg_outDeg_outside_temp[k] = avg_outDeg_outside_temp_temp
        avg_inDeg_outside_temp[k] = avg_inDeg_outside_temp_temp
        num_edges_inside_temp[k] = num_edges_inside_temp_temp
        num_edges_boundary_out_temp[k] = num_edges_boundary_out_temp_temp
        num_edges_boundary_in_temp[k] = num_edges_boundary_in_temp_temp
        num_edges_outside_temp[k] = num_edges_outside_temp_temp
        cluster_size_temp[k] = cluster_size_temp_temp
     # Adding things to dictionary to store results   
    measures_struct[families[i], '_rate_distortion_upper'] = rate_distortion_upper
    measures_struct[families[i], '_rate_distortion_lower'] = rate_distortion_lower
    measures_struct[families[i], '_avg_outDeg_inside'] = avg_outDeg_inside
    measures_struct[families[i], '_avg_inDeg_inside'] = avg_inDeg_inside
    measures_struct[families[i], '_avg_outDeg_outside'] = avg_outDeg_outside
    measures_struct[families[i], '_avg_inDeg_outside'] = avg_inDeg_outside
    measures_struct[families[i], '_num_edges_inside'] = num_edges_inside
    measures_struct[families[i], '_num_edges_boundary_out'] = num_edges_boundary_out
    measures_struct[families[i], '_num_edges_boundary_in'] = num_edges_boundary_in
    measures_struct[families[i], '_num_edges_outside'] = num_edges_outside
    measures_struct[families[i], '_cluster_size'] = cluster_size
    measures_struct[families[i], '_compressibility'] = compressibility
    measures_struct[families[i], '_avg_deg'] = avg_deg
    measures_struct[families[i], '_avg_clustering'] = avg_clustering
    measures_struct[families[i], '_inDeg_het'] = inDeg_het
